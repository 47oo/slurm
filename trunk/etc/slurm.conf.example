# 
#                   Sample configuration file for SLURM
#  $Id$

#
# This file holds the system-wide SLURM configuration. It is read
# by SLURM clients, daemons, and the SLURM API to determine where
# and how to contact the SLURM controller, what other nodes reside
# in the current cluster, and various other configuration information.
#
# SLURM configuration parameters take the form Keyword=Value, where
# at this time, no spacing is allowed to surround the equals (=) sign.
# Many of the config values are not mandatory, and so may be left
# out of the config file. We will attempt to list the default 
# values for those parameters in this file.

#  
#     SLURM daemon configuration
#     ========================================================================

#
# o Define the location of the SLURM controller and backup controller:
#    "ControlMachine"   : hostname of the primary controller
#    "ControlAddr"      : hostname used to contact the primary controller
#    "BackupController" : hostname of the backup controller 
#    "BackupAddr"       : hostname used to contact backup controller
#
# Example:
#
# ControlMachine=dev0
# ControlAddr=edev0		# default: same as ControlMachine
# BackupController=dev1		# default: no backup controller
# BackupAddr=edev1		# default: same as BackupController

#
# o Define the SLURM controller "save state" directory
#
#   The SLURM controller, slurmctld, will periodically save state
#   into this directory so that said state may be recovered after
#   a fatal system error. For best results when using a backup 
#   controller, the filesystem on which this directory resides 
#   should be shared between the "ControlMachine" and "BackupController"
#
# Example:
# 
# StateSaveLocation=/mnt/slurm	# default: "/tmp"


#
# o Define the slurmd "save state" directory
#
#   The SLURM daemon executing on each host, slurmd, will periodically 
#   save state into this directory so that said state may be recovered
#   after a fatal system error. This pathname is shared by all hosts, 
#   but the file must be unique on each host so this must reference a 
#   local file system.
#
# Example:
# 
# SlurmdSpoolDir=/tmp/slurmd	# default: "/tmp/slurmd"


#
# o Define the "slurm" user
#
#   "SlurmUser" specifies the user that the SLURM controller should run
#   as. The slurm controller has no need to run with elevated privileges,
#   so a user other than "root" is suggested here. 
#
# Example:
#
# SlurmUser=slurm


#
# o Define the slurmctld and slurmd server port numbers
#
#  by default, the slurmctld ports are set by checking for an entry in
#  /etc/services, and if that fails, by using an internal default set
#  at build time. That process will be overridden by these config 
#  parameters.
#
#    "SlurmctldPort"    : slurmctld server port 
#    "SlurmdPort"       : slurmd server port
#
# Example:
#
# SlurmctldPort=7010 	# default: @SLURMCTLD_PORT@
# SlurmdPort=7011       # default: @SLURMD_PORT@


#
# o Define slurmd and slurmctld logging options
#
#    "SlurmctldDebug"   : verbosity of slurmctld log messages 
#                         (Values from 0 to 7 are legal, with `0' being
#                          "quiet" operation and `7' being insanely verbose)
#
#    "SlurmdDebug"      : verbosity of slurmd log messages (0-7, see above)
#
#    "SlurmctldLogfile" : fully qualified pathname to slurmctld logfile
#                         (If a logfile is set for slurmctld, logging via
#                          syslog will be turned off)
#
#    "SlurmdLogfile"    : fully qualified pathname to slurmd logfile
#                         (same caveat as SlurmctldLogfile above)
#
# Example:
#
# SlurmctldDebug=4	# default is `3'  
# SlurmdDebug=4		# default is `3'
#
# SlurmctldLogFile=/var/log/slurmctld.log  # default is to log via syslog()
# SlurmdLogFile=/var/log/slurmd.log        # default is to log via syslog()


# o Define an alternate location for slurmd and slurmctld pid files
#  
#    "SlurmctldPidFile" : fully qualified pathname containing slurmctld pid
#
#    "SlurmdPidFile"    : fully qualified pathname containing slurmd pid
#    
# Example:
#
# SlurmctldPidFile=/var/slurm/slurmctld.pid  # default: "/var/run/slurmctld.pid"
# SlurmdPidFile=/var/slurm/slurmctld.pid     # default: "/var/run/slurmd.pid"


#
# o Define some timeout values for the slurm controller and backup
#
#    "SlurmctldTimeout" : amount of time, in seconds, backup controller
#                         waits for primary controller to respond 
#                         before assuming control.
#
#    "SlurmdTimeout"    : amount of time, in seconds, the controller
#                         waits for slurmd to respond before setting the
#                         node's state to DOWN.
#
#    "HeartbeatInterval": The interval, in seconds, at which the SLURM
#                         controller tests the status of other daemons.
#
#    "InactiveLimit"    : The interval, in seconds, a job is permitted to
#                         be inactive (no active job steps) before it is
#                         terminated.
#   
#    "KillWait"         : The time, in seconds, between SIGTERM and SIGKILL
#                         signals sent to a job upon reaching its timelimit.
#
# Example:
#
# SlurmctldTimeout=60	# Defaults to 300 seconds
# SlurmdTimeout=60	# Defaults to 300 seconds
# HeartBeatInterval=20	# Defaults to 30 seconds
# InactiveLimit=600	# Defaults to 0 (unlimited)
# KillWait=10		# Defaults to 30 seconds


#
# o Define other miscellaneous SLURM controller configuration values:
#
#    "FastSchedule"     : if set to `1' consider the configuration of nodes
#                         to be exactly that set in the config file. Otherwise,
#                         consider configuration of nodes to that which is
#                         reported by the node's slurmd. 
#
#    "FirstJobId"       : Number of the first assigned job id.
#
#    "HashBase"         : Base of numeric suffix on node hostnames
#                         (always `10' in a sane world)
#
#    "Prioritize"       : path to a script that will be used to set initial
#                         priorities on incoming jobs.
#
#    "ReturnToService"  : if set to `1,' nodes in the DOWN state will be
#                         set to IDLE after they come back up. Otherwise,
#                         nodes will stay in the down state until manually
#                         brought into the IDLE state.
# 
# Example:
#
# FastSchedule=0	# default is `1'
# FirstJobid=1000       # default is `1'
# HashBase=16		# default is `10'
# Prioritize=/etc/prio  # default is none
# ReturnToService=1     # default is `0'


#
# o Define an epilog and a prolog
#
#    "Prolog" : fully qualified path to script that will be executed as 
#               root on every node of a user's job before the job's tasks
#               will be initiated there.
#
#    "Epilog" : fully qualified path to a script that will be executed as
#               root on every node of a user's job after that job has 
#               terminated.
#
# Example:
#
# Prolog=/usr/local/slurm/prolog	# default is no prolog
# Epilog=/usr/local/slurm/epilog	# default is no epilog


#
# o Define the temporary file system 
#
#    "TmpFS"  : Defines the location of local temporary storage filesystem 
#               on remote nodes. This filesystem will be used in reporting
#               each node's TmpDisk space.
#
# Example:
#
# TmpFs=/var/tmp	# default "/tmp"


#
# o Define the location of the private and public keys used by SLURM
#   to generate job credentials.
#
#    "JobCredentialPrivateKey"       : Full pathname to the private key
#
#    "JobCredentialPublicCertificate : Full pathname to the public cert.
#
# Example:
#
# JobCredentialPrivateKey=/etc/slurm/slurm.key
# JobCredentialPublicCertificate=/etc/slurm/slurm.cert 


#
#     Node and Partition Configuration
#     ========================================================================

#
#  o Node configuration
#
#    The configuration information of nodes (or machines) to be managed 
#    by SLURM is described here. The only required value in this section
#    of the config file is the "NodeName" field, which specifies the 
#    hostnames of the node or nodes to manage. It is recommended, however,
#    that baseline values for the node configuration be established
#    using the following parameters (see slurm.config(5) for more info): 
#
#     "NodeName"   : The only required node configuration parameter, NodeName
#                    specifies a node or set of nodes to be managed by SLURM.
#                    The special NodeName of "DEFAULT" may be used to set up
#                    a list of default node configuration parameters
#                    (which are described below)
#
#     "Feature"    : comma separated list of "features" for the given node(s) 
#
#     "NodeAddr"   : preferred address for contacting the node
#
#     "RealMemory" : Amount of real memory (in Megabytes)
#
#     "Procs"      : Number of CPUs 
#
#     "State"      : Initial state (IDLE, DOWN, etc.)
#
#     "TmpDisk"    : Temporary disk space available on node
#
#     "Weight"     : Priority of node for scheduling purposes
#
#   If any of the above values are set for a node or group of nodes, and
#   that node checks in to the slurm controller with less than the 
#   configured resources, the node's state will be set to DOWN, in order
#   to avoid scheduling any jobs on a possibly misconfigured machine.
#
# Example Node configuration:
#
# NodeName=DEFAULT Procs=2 TmpDisk=64000 State=UNKNOWN
# NodeName=host[0-25] NodeAddr=ehost[0-25] Weight=16
# NodeName=host26     NodeAddr=ehost26     Weight=32 Feature=graphics_card

#
# o Partition Configuration
#
#   Paritions are groups of nodes which (possibly) have different limits
#   and access controls. Nodes may only be in one partition and jobs will
#   not be allowed to span partitions. The following partition configuration
#   parameters are recognized:
#
#    "PartitionName" : Name used to reference this partition
#
#    "Nodes"         : list of nodes that compose this partition
#
#    "AllowGroups"   : Comma separated list of group ids which are allowed
#                      to use the partition. Default is "ALL" which allows
#                      all users to access the partition.
#
#    "Default"       : if "YES" the corresponding partition will be the 
#                      default when users submit jobs without specification
#                      of a desired partition.
#
#    "RootOnly"      : only user id zero (root) may use this partition
#
#    "MaxNodes"      : Maximum count of nodes that will be allocated to any
#                      single job. The default is unlimited or `-1'
#
#    "MaxTime"       : Maximum timelimit of jobs in this partition
#
#    "Shared"        : Allow sharing of nodes by jobs. Possible values are
#                      "YES" "NO" or "FORCE"
#
#    "State"         : State of partition. Possible values are "UP" or "DOWN"
#
#
# Example Partition Configurations:
#
# PartitionName=DEFAULT MaxTime=30 MaxNodes=26
# PartitionName=debug Nodes=host[0-8,18-25] State=UP Default=YES
# PartitionName=batch Nodes=host[9-17,26]   State=UP
#
#

#
