.\" $Id$
.TH "slaunch" "1" "SLURM 1.2" "July 2006" "SLURM Commands"
.SH "NAME"
.LP 
slaunch \- Launch a parallel application under a SLURM job allocation.
.SH "SYNOPSIS"
.LP 
slaunch [\fIoptions\fP] <\fIcommand\fP> [\fIcommand args\fR]
.SH "DESCRIPTION"
.LP 
slaunch launches a parallel application (a \fBjob step\fR in SLURM parlance) on the nodes, or subset of nodes, in a \fBjob allocation\fR.   A valid job allocation is a prerequisite of running slaunch.  The ID of the job allocation may be passed to slaunch through either the \fB\-\-jobid\fR command line parameter or the \fBSLURM_JOBID\fR environment variable.  The \fBsalloc\fR and \fBsbatch\fR commands may be used to request a job allocation, and each of those commands automatically set the \fBSLURM_JOBID\fR environment variable.
.SH "OPTIONS"
.LP 
.TP 
\fB\-\-jobid\fR <\fIJOBID\fP>
The job allocation under which the parallel application should be launched.  If slaunch is running under salloc or a batch script, slaunch can automatically determint the jobid from the SLURM_JOB_ID environment variable.  Otherwise, you will need to tell slaunch which job allocation to use.
.TP 
\fB\-n\fR, \fB\-\-tasks\fR[=]<\fInumber\fR>
Specify the number of processes to launch.  The default is one process per node.
.TP 
\fB\-N\fR, \fB\-\-nodes\fR[=]<\fInumber\fR>
Specify the number of nodes to be used by this job step.  By default,
slaunch will use all of the nodes in the specified job allocation.
.TP 
\fB\-r\fR, \fB\-\-relative\fR[=]<\fInumber\fR>
Specify the first node in the allocation on which this job step will be launched.  Counting starts at zero, thus the first node in the job allocation is node 0.  The option to \-\-relative may also be a negative number.  \-1 is the last node in the allocation, \-2 is the next to last node, etc.  By default, the controller will select the starting node (assuming that there are no other nodelist or task layout options that specify specific nodes).

.TP 
\fB\-c\fR, \fB\-\-cpus\-per\-task\fR[=]<\fIncpus\fR>
Specify that each task requires \fIncpus\fR number of CPUs.  Useful for applications in which each task will launch multiple threads and can therefore benefit from there being free processors on the node.

.TP 
\fB\-w\fR, \fB\-\-nodelist\-byname\fR[=]<\fInode name list\fR>
Request a specific list of node names.  The list may be specified as a comma\-separated list of node names, or a range of node names (e.g. mynode[1\-5,7,...]).  Duplicate node names are not permitted in the list.
The order of the node names in the list is not important; the node names
will be sorted my SLURM.
.TP 
\fB\-L\fR, \fB\-\-nodelist\-byid\fR[=]<\fInode index list\fR>
Request a specific set of nodes in a job alloction on which to run the tasks of the job step.  The list may be specified as a comma\-separated list relative node indices in the job allocation (e.g., "0,2\-5,\-2,8").  Duplicate indices are permitted, but are ignored.  The order of the node indices in the list is not important; the node indices will be sorted my SLURM.

.TP 
\fB\-T\fR, \fB\-\-task\-layout\-byid\fR[=]<\fInode index list\fR>
Request a specific task layout using node indices within the job allocation.  The node index list can contain duplicate indices, and the indices may appear in any order.  The order of indices in the nodelist IS significant.  Each node index in the list represents one task, with the Nth node index in the list designating on which node the Nth task should be launched.

For example, given an allocation of nodes "linux[0\-15]" and a node index list "4,\-1,1\-3" task 0 will run on "linux4", task 1 will run on "linux15", task 2 on "linux1", task 3 on "linux2", and task 4 on "linux3".

NOTE: This option implicitly sets the task distribution method to "arbitrary".  Some network switch layers do not permit arbitrary task layout.

.TP 
\fB\-Y\fR, \fB\-\-task\-layout\-byname\fR[=]<\fInode name list\fR>
Request a specific task layout.  The nodelist can contain duplicate node
names, and node names may appear in any order.  The order of node names in
the nodelist IS significant.  Each node name in the nodes list represents
one task, with the Nth node name in the nodelist designating on which node
the Nth task should be launched.  For example, a nodelist of mynode[4,3,1\-2,4]
means that tasks 0 and 4 will run on mynode4, task 1 will run on mynode3,
task 2 will run on mynode1, and task 3 will run on mynode2.

NOTE: This option implicitly sets the task distribution method to "arbitrary".  Some network switch layers do not permit arbitrary task layout.
.TP 
\fB\-F\fR, \fB\-\-task\-layout\-file\fR[=]<\fIfilename\fR>
Request a specific task layout.  This options much like the \-\-task\-layout option, except that instead of a nodelist you supply the name of a file.  The file contains a nodelist that may span multiple lines of the file.

NOTE: This option implicitly sets the task distribution method to "arbitrary".  Some network switch layers do not permit arbitrary task layout.

.TP 
\fB\-i\fR, \fB\-\-slaunch\-input\fR[=]<\fIfilename pattern\fR>
.PD 0 
.TP
\fB\-o\fR, \fB\-\-slaunch\-output\fR[=]<\fIfilename pattern\fR>
.PD 0
.TP 
\fB\-e\fR, \fB\-\-slaunch\-error\fR[=]<\fIfilename pattern\fR>
.PD
Change slaunch's standard input, standard output, or standard error
to be a file of name "filename pattern".  These options are similar to using
shell IO redirection capabilities, but with the additional ability to replace
certain symbols in the filename with useful SLURM information.  Symbols are
listed below.

By default, slaunch broadcasts its standard input over the network to the
standard input of all tasks.  Likewise, standard output and standard error
from all tasks are collected over the network by slaunch and printed on
its standard output or standard error, respectively.  If you want to see
traffic from fewer tasks, see the \-\-slaunch\-[input|output|error]\-filter
options.

Supported replacement symbols are:
.PD 0
.RS 10
.TP 
\fB%J\fR
Job allocation number and job step number in the form "jobid.stepid".  For instance, "128.0".
.PD 0
.TP 
\fB%j\fR
Job allocation number.
.PD 0
.TP 
\fB%s\fR
Job step number.
.RS -10

.TP 
\fB\-\-slaunch\-input\-filter\fR[=]<\fItask number\fR>
.PD 0
.TP
\fB\-\-slaunch\-output\-filter\fR[=]<\fItask number\fR>
.PD 0
.TP 
\fB\-\-slaunch\-error\-filter\fR[=]<\fItask number\fR>
.PD
Only transmit standard input to a single task, or print the standard output
or standard error from a single task.  These options perform the filtering
locally in slaunch.  All tasks are still capable of sending or receiving
standard IO over the network, so the "sattach" command can still access the
standard IO streams of the other tasks.  (NOTE: for -output and -error,
the streams from all tasks WILL be transmitted to slaunch, but it will only
print the streams for the selected task.  If your tasks print a great deal of
data to standard output or error, this can be performance limiting.)

.TP
\fB\-I\fR, \fB\-\-task\-input\fR[=]<\fIfilename pattern\fR>
.PD 0
.TP
\fB\-O\fR, \fB\-\-task\-output\fR[=]<\fIfilename pattern\fR>
.PD 0
.TP
\fB\-E\fR, \fB\-\-task\-error\fR[=]<\fIfilename pattern\fR>
.PD
Instruct SLURM to connect each task's standard input, standard output,
or standard error directly to the file name specified
in the "\fIfilename pattern\fR".

By default, the standard IO streams of all tasks are received and transmitted
over the network to commands like slaunch and sattach.  These options disable
the networked standard IO streams and instead connect the standard IO streams
of the tasks directly to files on the local node of each task (although the file
may, of course, be located on a networked filesystem).

Whether or not the tasks share a file depends on whether or not the file lives
on a local filesystem or a shared network filesytem, and on whether or not
the filename pattern expands to the same file name for each task.

The filename pattern may
contain one or more replacement symbols, which are a percent sign "%" followed 
by a letter (e.g. %t).

Supported replacement symbols are:
.PD 0
.RS 10
.TP 
\fB%J\fR
Job allocation number and job step number in the form "jobid.stepid".  For instance, "128.0".
.PD 0
.TP 
\fB%j\fR
Job allocation number.
.PD 0
.TP 
\fB%s\fR
Job step number.
.PD 0
.TP 
\fB%N\fR
Node name. (Will result in a separate file per node.)
.PD 0
.TP 
\fB%n\fR
Relative node index number within the job step.  All nodes used by the job step will be number sequentially starting at zero.  (Will result in a separate file per node.)
.PD 0
.TP 
\fB%t\fR
Task rank number.  (Will result in a separate file per task.)
.RS -10

.TP 
\fB\-D\fR, \fB\-\-workdir\fR=\fIpath\fR
Set the working directory of the tasks to \fIpath\fR before execution.
The default task working directory is slaunch's working directory.

.TP 
\fB\-\-nice\fR=<\fIadjustment\fR>
Run  the  job  with  an  adjusted  scheduling priority.  With no adjustment value the scheduling priority is  decreased  by  100.  The adjustment range is from \-10000 (highest priority) to 10000 (lowest priority). Only privileged users can specify a  negative adjustment. NOTE: This option is presently ignored if SchedulerType=sched/maui.


.TP 
\fB\-h\fR, \fB\-\-help\fR
Output help information and exit.
.TP 
\fB\-V\fR, \fB\-\-version\fR
Output version information and exit.
.SH "ENVIRONMENT VARIABLES"
.LP 
.TP 
\fBSLAUNCH_JOBID\fP
Same as \fB\-\-jobid\fR.
.SH "EXAMPLES"
.LP 
To launch a job step (parallel program) in an existing job allocation:
.IP 
slaunch \-\-jobid 66777 \-N2 \-n8 myprogram
.LP 
To grab an allocation of nodes and launch a parallel application on one command line (See the \fBsalloc\fR man page for more examples):
.IP 
salloc \-N5 slaunch \-n10 myprogram
.SH "SEE ALSO"
.LP 
sinfo(1), salloc(1), sbatch(1), squeue(1), scancel(1), scontrol(1), slurm.conf(5), sched_setaffinity(2), numa(3)
