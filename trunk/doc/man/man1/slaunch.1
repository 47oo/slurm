.\" $Id$
.TH "slaunch" "1" "SLURM 1.2" "July 2006" "SLURM Commands"
.SH "NAME"
.LP 
slaunch \- Launch a parallel application under a SLURM job allocation.
.SH "SYNOPSIS"
.LP 
slaunch [\fIoptions\fP] <\fIcommand\fP> [\fIcommand args\fR]
.SH "DESCRIPTION"
.LP 
slaunch launches a parallel application (a \fBjob step\fR in SLURM parlance) on the nodes, or subset of nodes, in a \fBjob allocation\fR.   A valid job allocation is a prerequisite of running slaunch.  The ID of the job allocation may be passed to slaunch through either the \fB\-\-jobid\fR command line parameter or the \fBSLURM_JOBID\fR environment variable.  The \fBsalloc\fR and \fBsbatch\fR commands may be used to request a job allocation, and each of those commands automatically set the \fBSLURM_JOBID\fR environment variable.
.SH "OPTIONS"
.LP 
.TP 
\fB\-\-jobid\fR <\fIJOBID\fP>
The job allocation under which the parallel application should be launched.  If slaunch is running under salloc or a batch script, slaunch can automatically determint the jobid from the SLURM_JOB_ID environment variable.  Otherwise, you will need to tell slaunch which job allocation to use.
.TP 
\fB\-n\fR, \fB\-\-tasks\fR[=]<\fInumber\fR>
Specify the number of processes to launch.  The default is one process per node.
.TP 
\fB\-N\fR, \fB\-\-nodes\fR[=]<\fInumber\fR>
Specify the number of nodes to be used by this job step.  By default,
slaunch will use all of the nodes in the specified job allocation.
.TP 
\fB\-r\fR, \fB\-\-relative\fR[=]<\fInumber\fR>
Specify the first node in the allocation on which this job step will be launched.  Counting starts at zero, thus the first node in the job allocation is node 0.  The option to \-\-relative may also be a negative number.  \-1 is the last node in the allocation, \-2 is the next to last node, etc.  By default, the controller will select the starting node (assuming that there are no other nodelist or task layout options that specify specific nodes).

.TP 
\fB\-c\fR, \fB\-\-cpus\-per\-task\fR[=]<\fIncpus\fR>
Specify that each task requires \fIncpus\fR number of CPUs.  Useful for applications in which each task will launch multiple threads and can therefore benefit from there being free processors on the node.

.TP 
\fB\-w\fR, \fB\-\-nodelist\-byname\fR[=]<\fInode name list\fR>
Request a specific list of node names.  The list may be specified as a comma\-separated list of node names, or a range of node names (e.g. mynode[1\-5,7,...]).  Duplicate node names are not permitted in the list.
The order of the node names in the list is not important; the node names
will be sorted my SLURM.
.TP 
\fB\-L\fR, \fB\-\-nodelist\-byid\fR[=]<\fInode index list\fR>
Request a specific set of nodes in a job allaction for use by the job step.  The list may be specified as a comma\-separated list relative node indices in the job allocation (e.g., "0,2\-5,\-2,8").  Duplicate indices are permitted, but are ignored.  The order of the node indices in the list is not important; the node indices will be sorted my SLURM.

.TP 
\fB\-T\fR, \fB\-\-task\-layout\-byid\fR[=]<\fInode index list\fR>
Request a specific task layout using node indices within the job allocation.  The node index list can contain duplicate indices, and the indices may appear in any order.  The order of indices in the nodelist IS significant.  Each node index in the list represents one task, with the Nth node index in the list designating on which node the Nth task should be launched.

For example, given an allocation of nodes "linux[0\-15]" and a node index list "4,\-1,1\-3" task 0 will run on "linux4", task 1 will run on "linux15", task 2 on "linux1", task 3 on "linux2", and task 4 on "linux3".

NOTE: This option implicitly sets the task distribution method to "arbitrary".  Some network switch layers do not permit arbitrary task layout.

.TP 
\fB\-Y\fR, \fB\-\-task\-layout\-byname\fR[=]<\fInode name list\fR>
Request a specific task layout.  The nodelist can contain duplicate node
names, and node names may appear in any order.  The order of node names in
the nodelist IS significant.  Each node name in the nodes list represents
one task, with the Nth node name in the nodelist designating on which node
the Nth task should be launched.  For example, a nodelist of mynode[4,3,1\-2,4]
means that tasks 0 and 4 will run on mynode4, task 1 will run on mynode3,
task 2 will run on mynode1, and task 3 will run on mynode2.

NOTE: This option implicitly sets the task distribution method to "arbitrary".  Some network switch layers do not permit arbitrary task layout.

.TP 
\fB\-F\fR, \fB\-\-task\-layout\-file\fR[=]<\fIfilename\fR>
Request a specific task layout.  This options much like the \-\-task\-layout option, except that instead of a nodelist you supply the name of a file.  The file contains a nodelist that may span multiple lines of the file.

NOTE: This option implicitly sets the task distribution method to "arbitrary".  Some network switch layers do not permit arbitrary task layout.
.TP 
\fB\-\-nice\fR=<\fIadjustment\fR>
Run  the  job  with  an  adjusted  scheduling priority.  With no adjustment value the scheduling priority is  decreased  by  100.  The adjustment range is from \-10000 (highest priority) to 10000 (lowest priority). Only privileged users can specify a  negative adjustment. NOTE: This option is presently ignored if SchedulerType=sched/maui.


.TP 
\fB\-h\fR, \fB\-\-help\fR
Output help information and exit.
.TP 
\fB\-V\fR, \fB\-\-version\fR
Output version information and exit.
.SH "ENVIRONMENT VARIABLES"
.LP 
.TP 
\fBSLURM_JOBID\fP
Same as \fB\-\-jobid\fR.
.SH "EXAMPLES"
.LP 
To launch a job step (parallel program) in an existing job allocation:
.IP 
slaunch \-\-jobid 66777 \-N2 \-n8 myprogram
.LP 
To grab an allocation of nodes and launch a parallel application on one command line (See the \fBsalloc\fR man page for more examples):
.IP 
salloc \-N5 slaunch \-n10 myprogram
.SH "SEE ALSO"
.LP 
sinfo(1), salloc(1), sbatch(1), squeue(1), scancel(1), scontrol(1), slurm.conf(5), sched_setaffinity(2), numa(3)
