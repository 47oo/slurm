.\" $Id: salloc.1 8570 2006-07-13 21:12:58Z morrone $
.TH "sbatch" "1" "SLURM 1.2" "July 2006" "SLURM Commands"
.SH "NAME"
.LP 
sbatch \- Submit a batch script to SLURM.
.SH "SYNOPSIS"
.LP 
sbatch [\fIoptions\fP] [\fIscript\fP]
.SH "DESCRIPTION"
.LP 
sbatch submits a batch script to SLURM.  The batch script may be given to sbatch through a file name on the command line, or if no file name is specified, sbatch will read in a script from standard input.

sbatch exits immediately after the script is successfully transferred to the SLURM controller and assigned a SLURM job ID.  The batch script is not necessarily granted resources immediately, it may sit in the queue of pending jobs for some time before its required resources become available.

When the job allocation is finally granted for the batch script, SLURM runs a single copy of the batch script on the first node in the set of allocated nodes.
.SH "OPTIONS"
.LP 
.TP 
\fB\-N\fR, \fB\-\-nodes\fR[=]<\fInumber|[min]\-[max]\fR>
Specify the number of nodes to be used by this job step.  This option accepts either a single number, or a range of possible node counts.  If a single number is used, such as "\-N 4", then the allocation is asking for four and ONLY four nodes.  If a range is specified, such as "\-N 2\-6", SLURM controller may grant salloc anywhere from 2 to 6 nodes.  When using a range, either of the min or max options may be omitted.  For instance, "\-N 10\-" means "no fewer than 10 nodes", and "\-N \-20" means "no more than 20 nodes".  The default value of this option is one node, but other options may require more than one node to be allocated.
.TP 
\fB\-D\fR, \fB\-\-dependency\fR[=]<\fIjobid\fR>
Defer the start of this job until the specified \fIjobid\fR has completed.  Many jobs can share the same dependency and these jobs may even belong to different  users.   The  value may be changed after job submission using the scontrol command.
.TP 
\fB\-I\fR,\fB\-\-immediate\fR
Do not wait for the resources need to grant this allocation.  Normally salloc will wait for the resources necessary to satisfy the requested job allocation, but when \-\-immediate is specified it will exit immediately without running the \fIcommand\fR parameter.

.TP 
\fB\-p\fR, \fB\-\-partition\fR[=]<\fIpartition name\fR>
Request a specific partition for the resource allocation.  If not specified, the default behaviour is to allow the slurm controller to select the default partition as designated by the system administrator.
.TP 
\fB\-c\fR, \fB\-\-cpus\-per\-task\fR=<\fIncpus\fR>

.TP 
\fB\-\-help\fR
Output help information and exit.
.TP 
\fB\-V\fR, \fB\-\-version\fR
Output version information and exit.
.SH "ENVIRONMENT VARIABLES"
.LP 
.SH "EXAMPLES"
.LP 
Specify a batch script by filename on the command line:
.IP 
$ cat myscript
#!/bin/sh
slaunch hostname |sort
$ sbatch \-N4 myscript
salloc: Granted job allocation 65537
$ cat slurm\-65537.out
host1
host2
host3
host4
.LP 
Pass a batch script to sbatch on standard input:
.IP 
morrone:~$ sbatch \-N4 <<EOF
> #!/bin/sh
> slaunch hostname |sort
> EOF
sbatch: Submitted batch job 65541
$ cat slurm\-65541.out
host1
host2
host3
host4
.SH "SEE ALSO"
.LP 
sinfo(1), slaunch(1), salloc(1), squeue(1), scancel(1), scontrol(1), slurm.conf(5), sched_setaffinity(2), numa(3)
