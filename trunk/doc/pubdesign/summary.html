<html>
<head>
<title>SLURM: A Highly Scalable Resource Manager for Linux Clusters</title>
</head>
<body>
<h1>SLURM: A Highly Scalable Resource Manager for Linux Clusters</h1>
<a href="http://www.llnl.gov/">Lawrence Livermore National Laboratory (LLNL)</a> 
and <a href="http://www.lnxi.com/">Linux Networx</a> 
are designing and devoping SLURM, Simple Linux Utility for 
Resource Management. 
SLURM provides three key functions:
First, it allocates exclusive and/or non-exclusive access to 
resources (compute nodes) to users for some duration of time 
so they can perform work. 
Second, it provides a framework for starting, executing, and 
monitoring work (typically a parallel job) on a set of allocated 
nodes. 
Finally, it arbitrates conflicting requests for resources by 
managing a queue of pending work.
SLURM is not a sophisticated batch system, but it does provide 
an Applications Programming Interface (API) for integration 
with external schedulers such as 
<a href="http://mauischeduler.sourceforge.net/">The Maui Scheduler</a>.
While other resources managers do exist, SLURM is unique in 
several respects: 
<ul>
<li>It's source code is freely available under the 
<a href="http://www.gnu.org/licenses/gpl.html">GNU General 
Public License</a>.
<li>It is designed to operate in a heterogeneous cluster with 
up to thousands of nodes.
<li>It is portable; written in C with a GNU <i>autoconf</i> configuration 
engine. While initially written for Linux, other UNIX-like 
operating systems should be easy porting targets. The interconnect 
to be initially supported is <a href="http://www.quadrics.com/">
Quadrics</a> Elan3, but support for other interconnects is already planned.
<li>SLURM is highly tolerant of system failures including failure 
of the node executing its control functions.
<li>It is simple enough for the motivated end user to understand 
its source and add functionality.
</ul>
<h2>Architecture</h2>
SLURM has a centralized manager, <i>slurmctld</i>, to monitor 
resources and work. 
There may also be a backup manager to assume those responsibilities 
in the event of failure. 
Each compute server (node) has a <i>slurmd</i> daemon, which can be 
compared to a remote shell: it waits for work, executes that work, 
returns status, and waits for more work. 
User tools include <i>srun</i> to initiate jobs, 
<i>scancel</i> to terminate queued or running jobs, and 
<i>squeue</i> to report the status of jobs.
There is also an administrative tool <i>scontrol</i> available to 
monitor and/or modify configuration and state information. 
APIs are available for all functions. 
Security is supported with the use of Pluggable Authentication 
Modules (PAM), which is presently interfaced with 
<a href=http://www.cs.berkeley.edu/~bnc/authd>authd</a>
to provide flexible user authentication.
<p align=center>
<img src="figures/arch.gif">
</p>
<h2>Configurability</h2>
Node state monitored include: count of processors, size of real memory, 
size of temporary disk space, and state (UP, DOWN, etc.). 
Additional node information includes weight (preference in being allocated 
work) and features (arbitrary information such as processor speed or type). 
Nodes are grouped into disjoint partitions. 
Partition information includes: name, list of associated nodes, 
state (UP or DOWN), maximum job time limit, maximum node count per job, 
group access list, and shared node access (YES, NO or FORCE).
Jobs are allocated nodes from a single partition. 
A sample (partial) SLURM configuration file follows.
<pre>
# 
# Sample /etc/slurm.conf
#
ControlMachine=linux0001.llnl.gov BackupController=linux0002.llnl.gov
Epilog=/usr/local/slurm/epilog Prolog=/usr/local/slurm/prolog
SlurmctldPort=7002 SlurmdPort=7003
StateSaveLocation=/usr/local/slurm/slurm.state
TmpFS=/tmp
#
# Node Configurations
#
NodeName=DEFAULT TmpDisk=16384 State=IDLE
NodeName=lx[0001-0002] State=DRAINED
NodeName=lx[0003-8000] Procs=16 RealMemory=2048 Weight=16
NodeName=lx[8001-9999] Procs=32 RealMemory=4096 Weight=40 Feature=1200MHz
#
# Partition Configurations
#
PartitionName=DEFAULT MaxTime=30 MaxNodes=2
PartitionName=login Nodes=lx[0001-0002] State=DOWN
PartitionName=debug Nodes=lx[0003-0030] State=UP    Default=YES
PartitionName=class Nodes=lx[0031-0040] AllowGroups=students
PartitionName=batch Nodes=lx[0041-9999] MaxTime=UNLIMITED MaxNodes=4096
</pre>

<h2>Status</h2>
As of August 2002 basic SLURM functionality was available, although much work 
remains for production use in the areas of fault-tolerance, Quadrics Elan3 integration, 
and security. We plan to have these issues fully addressed by November of 2002 
and have the system deployed in a production environment. Our next goal will be 
the support of the <a href="http://www.research.ibm.com/bluegene/">IBM Blue Gene/L</a>
architecture in the summer of 2003. For additional information please contact
<a href="mailto:jette@llnl.gov">jette@llnl.gov</a>.
<hr>
<a href="http://www.llnl.gov/disclaimer.html">Privacy and Legal Notice</a>
<p>URL = http://www-lc.llnl.gov/dctg-lc/slurm/summary.html
<p>Last Modified July 29, 2002</p>
<address>Maintained by Moe Jette <a href="mailto:jette@llnl.gov">
jette1@llnl.gov</a></address>
</body>
</html>
