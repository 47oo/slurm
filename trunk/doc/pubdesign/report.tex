\maketitle

\begin{abstract}
Abstract here.
\end{abstract}

\newpage

\section{Review of Related work}

\subsection{PBS (Portable Batch System)}

The Portable Batch System (PBS)\footnote{http://www.openpbs.org/}
is a flexible batch queuing and 
workload management system originally developed by Veridian Systems 
for NASA.  It operates on networked, multi-platform UNIX environments, 
including heterogeneous clusters of workstations, supercomputers, and 
massively parallel systems. PBS was developed as a replacement for 
NQS (Network Queuing System) by many of the same people.

PBS has sophisticated scheduling logic. It spawn's daemons on each 
machine to shepherd the job's tasks (appears to be like LoadLeveler 
and Condor). It provides an interface for administrators to easily 
interface their own scheduling modules (nice feature).  Can support 
long delays in file staging (in and out) with retry.  Host 
authentication provided by checking port number (low ports only 
accessible to root).  Credential service used for user authentication. 
It has the job prolog and epilog feature, which is useful.  Supports 
high priority queue for smaller "interactive" jobs.  Signal to daemons 
causes current log file (e.g. accounting) to be closed, renamed with 
time-stamp, and a new log file created.

Specific complaints about PBS from members of the OSCAR group (Jeremy Enos, 
Jeff Squyres, Tim Mattson):
\begin{itemize}
\item Sensitivity to hostname configuration on the server; improper 
      configuration results in hard to diagnose failure modes.  Once 
      configuration is correct, this issue dissappears.
\item When a compute node in the system dies, everything slows down.  
      PBS is single-threaded and continues to try to contact down nodes,
      while other activities like scheduling jobs, answering qsub/qstat 
      requests, etc., have to wait for a complete timeout cycle before being
      processed.
\item Default schduler is just FIFO, but Maui can be plugged in so this
      is not a big issue.
\item Weak mechanism for starting/cleaning up parallel jobs (pbsdsh).
      When a job is killed, pbsdsh kills the processes it started, but
      if the process doesn't die on the first shot it may continue on.
\item PBS server continues to mark specific nodes offline, even though they 
      are healthy.  Restarting the server fixes this.
\item Lingering jobs.  Jobs assigned to nodes, and then bounced back to the 
      queue for any reason, maintain their assignment to those nodes, even 
      if another job had already started on them.  This is a poor clean up 
      issue.
\item When the PBS server process is restarted, it puts running jobs at risk.
\item Poor diagnostic messages.  This problem can be as serious as ANY other 
      problem.  This problem makes small, simple problems turn into huge 
      turmoil occasionally.  For example, the variety of symptoms that arise 
      from improper hostname configuration.  All the symptoms that result are 
      very misleading to the real problem.
\item Rumored to have problems when the number of jobs in the queues gets
      large.
\item Scalability problems on large systems.
\item Non-portable to Windows
\item Source code is a mess and difficult for others (e.g. the open source
      community) to improve/expand.
\item Rumored to be moving away from an open source license.
\end{itemize}
The one strength mentioned is PBS's portability and broad user base.

\subsection{Maui}

Maui Scheduler\footnote{http://mauischeduler.sourceforge.net/}
is an advance reservation HPC batch scheduler for use with SP, 
O2K, and UNIX/Linux clusters. It is widely used to extend the 
functionality of PBS and Loadleveler

\subsection{DPCS}

Something about DPCS here for peer reviewers.

\subsection{LoadLeveler}

LoadLeveler\footnote{
http://www-1.ibm.com/servers/eserver/pseries/software/sp/loadleveler.html}
 is a proprietary batch system and parallel job manager by 
IBM. LoadLeveler supports few non-IBM systems. Very primitive 
scheduling software dependent upon other software for reasonable 
performance (e.g. Maui and DPCS). Many soft and hard limits. Very 
flexible queue and job class structure operating in "matrix" fashion 
(probably overly complex). Many configuration files with signals to 
daemons used to update configuration (like LSF, good). All jobs must 
be initiated through LoadLeveler (no real "interactive" jobs, just 
high priority queue for smaller jobs). Job accounting only available 
on termination (very bad for long-running jobs). Good status 
information on nodes and LSF daemons. Allocates jobs either entire 
nodes or shared nodes depending upon configuration.

A special version of MPI is required. LoadLeveler allocates 
interconnect resources, spawns the user's processes, and manages the 
job afterwards. Daemons also monitor the switch and node health using 
a "heart-beat monitor." One fundamental problem is that when the 
"Central Manager" restarts, it forgets about all nodes and jobs. They 
appear in the database only after checking in via the heartbeat. It 
needs to periodically write state to disk instead of doing 
"cold-starts" after the daemon fails, which is rare. It has the job 
prolog and epilog feature, which permits us to enable/disable logins 
and remove stray processes.

LoadLeveler evolved from Condor, or what was Condor a decade ago. 
While I am less familiar with LSF and Condor than LoadLeveler, they 
all appear very similar with LSF having the far more sophisticated 
scheduler. We should carefully review their data structures and 
daemons before designing our own.

\subsection{LSF (Load Sharing Facility)}


LSF\footnote{http://www.platform.com/}
 is a proprietary batch system and parallel job manager by 
Platform Computing. Widely deployed on a wide variety of computer 
architectures. Sophisticated scheduling software including 
fair-share, backfill, consumable resources, job preemption, many soft 
and hard limits, etc. Very flexible queue structure (perhaps overly 
complex). Most limits are per process rather than per-job (not very 
useful). Time limits include CPU time and wall-clock time. Many 
configuration files with signals to daemons used to update 
configuration (like LoadLeveler, good). All jobs must be initiated 
through LSF (no real "interactive" jobs, just high priority queue for 
smaller jobs). Job accounting only available on termination (very bad 
for long-running jobs). Jobs initiated from same directory as 
submitted from (not good for computer centers with diverse systems 
under LSF control). Good status information on nodes and LSF daemons. 
Allocates jobs either entire nodes or shared nodes depending upon 
configuration.

A special version of MPI is required. LSF allocates interconnect 
resources, spawns the user's processes, and manages the job 
afterwards. While I am less familiar with LSF than LoadLeveler, they 
appear very similar with LSF having the far more sophisticated 
scheduler. We should carefully review their data structures and 
daemons before designing our own.


\subsection{Condor}


Condor\footnote{http://www.cs.wisc.edu/condor/}
 is an open source batch system and parallel job manager 
developed by the University of Wisconsin. Condor was the basis for 
IBM's LoadLeveler and both share very similar underlying 
infrastructure. Condor has a very sophisticated checkpoint/restart 
service that does not rely upon kernel changes, but a variety of 
library changes (which prevent it from being completely general). The 
Condor checkpoint/restart service has been integrated into LSF, 
Codine, and DPCS. Condor is designed to operate across a 
heterogeneous environment, mostly to harness the compute resources of 
workstations and PCs. It has an interesting "advertising" service. 
Servers advertise their available resources and consumers advertise 
their requirements for a broker to perform matches. The checkpoint 
mechanism is used to relocate work on demand (when the "owner" of a 
desktop machine wants to resume work).



\subsection{Memory Channel (Compaq)}

Memory Channel is a high-speed interconnect developed by 
Digital/Compaq with related software for parallel job execution. 
Special version of MPI required. The application spawns tasks on 
other nodes. These tasks connect themselves to the high speed 
interconnect. No system level tool to spawns the tasks, allocates 
interconnect resources, or otherwise manages the parallel job (Note: 
This is sometimes a problem when jobs fail, requiring system 
administrators to release interconnect resources. There are also 
performance problems related to resource sharing).

\subsection{Linux PAGG Process Aggregates}


PAGG\footnote{http://oss.sgi.com/projects/pagg/}
consists of modifications to the linux kernel that allows
developers to implement Process AGGregates as loadable kernel modules.
A process aggregate is defined as a collection of processes that are
all members of the same set. A set would be implemented as a container
for the member processes. For instance, process sessions and groups
could have been implemented as process aggregates.

\subsection{BPROC}


The Beowulf Distributed Process Space 
(BProc\footnote{http://bproc.sourceforge.net/})
is set of kernel
modifications, utilities and libraries which allow a user to start
processes on other machines in a Beowulf-style cluster.  Remote
processes started with this mechanism appear in the process table
of the front end machine in a cluster. This allows remote process
management using the normal UNIX process control facilities. Signals
are transparently forwarded to remote processes and exit status is
received using the usual wait() mechanisms.

\subsection{xcat}

Presumably IBM's suite of cluster management software 
(xcat\footnote{http://publib-b.boulder.ibm.com/Redbooks.nsf/RedbookAbstracts/sg246041.html})
includes a batch system.  Look into this.

\subsection{CPLANT}

CPLANT\footnote{http://www.cs.sandia.gov/cplant/} includes
Parallel Job Launcher, Compute Node Daemon Process,
Compute Node Allocator, Compute Node Status Tool.

\subsection{NQS} 

NQS\footnote{http://umbc7.umbc.edu/nqs/nqsmain.html}, 
the Network Queueing System, is a serial batch system.

\subsection{LAM / MPI}

LAM (Local Area Multicomputer)\footnote{http://www.lam-mpi.org/}
is an MPI programming environment and development system for heterogeneous 
computers on a network. 
With LAM, a dedicated cluster or an existing network
computing infrastructure can act as one parallel computer solving
one problem.  LAM features extensive debugging support in the
application development cycle and peak performance for production
applications. LAM features a full implementation of the MPI
communication standard.

\subsection{MPICH}

MPICH\footnote{http://www-unix.mcs.anl.gov/mpi/mpich/}
is a freely available, portable implementation of MPI,
the Standard for message-passing libraries.

\subsection{Quadrics RMS}

Quadrics
RMS\footnote{http://www.quadrics.com/downloads/documentation/}
(Resource Management System) is a cluster management system for 
Linux and Tru64 which supports the
Elan3 interconnect.  

\subsection{Sun Grid Engine}

SGE\footnote{http://www.sun.com/products-n-solutions/edu/hpc/presentations/june01/omar\_hassaine.pdf}


\subsection{SCIDAC}

The Scientific Discovery through Advanced Computing (SciDAC) 
project\footnote{http://www.scidac.org/ScalableSystems}
has a Resource Management and Accounting working group
and a white paper\cite{Res2000}.

\newpage
\bibliographystyle{plain}
\bibliography{project}
