<!--#include virtual="header.txt"-->

<h1>SLURM Troubleshooting Guide</h1>

<p>This guide is meant as a tool to help system administrators
or operators troubleshoot SLURM failures and restore services.</p>

<ul>
<li><a href="#resp">SLURM is not responding</a></li>
<li><a href="#sched">Jobs are not getting scheduled</a></li>
<li><a href="#nodes">Notes are getting set to a DOWN state</a></li>
<li><a href="#network">Networking problems</a></li>
</ul>

<h2><a name="resp">SLURM is not responding</a></h2>

<ol>
<li>Execute "<i>scontrol ping</i>" to determine if the primary 
and backup controllers are responding.
<li>If it responds for you, this could be a <a href="#network">networking 
or configuration problem</a> specific to some user or node in the 
cluster.</li> 
<li>If not responding, directly login to the machine and try again
to rule out <a href="#network">network and configuration problems</a>.
<li>If still not responding, check if there is an active slurmctld 
dameon by executing "<i>ps -el | grep slurmctld</i>".</li>
<li>If slurmctld is not running, restart it (typically as user root
using the command "<i>/etc/init.d/slurm start</i>").
You should check the log file (<i>SlurmctldLog</i> in the 
<i>slurm.conf</i> file) for an indication of why it failed.
If it keeps failing, you should contact the slurm team for help at
<a href="mailto:slurm-dev@lists.llnl.gov">slurm-dev@lists.llnl.gov</a>.</li>
<li>If slurmctld is running but not responding (a very rare situation), 
then kill and restart it (typically as user root using the commands 
"<i>/etc/init.d/slurm stop</i>" and then "<i>/etc/init.d/slurm start</i>").
<li>If it hangs again, increase the verbosity of debug messages
(increase <i>SlurmctldDebug</i> in the <i>slurm.conf</i> file) 
and restart.  
Again check the log file for an indication of why it failed.
At this point, you should contact the slurm team for help at
<a href="mailto:slurm-dev@lists.llnl.gov">slurm-dev@lists.llnl.gov</a>.</li>
<li>If it continues to fail without an indication as to the failure 
mode, restart without preserving state (typically as user root 
using the commands "<i>/etc/init.d/slurm stop</i>" 
and then "<i>/etc/init.d/slurm startclean</i>").
Note: All running jobs and other state information will be lost.</li>
</ol>

<h2><a name="sched">Jobs are not getting scheduled</a></h2>

<h2><a name="nodes">Notes are getting set to a DOWN state</a></h2>

<h2><a name="network">Networking and configuration problems</a></h2>

<ol>
<li>Check the controller and/or slurmd log files (<i>SlurmctldLog</i>
and <i>SlurmdLog</i> in the <i>slurm.conf</i> file) for an indication 
of why it is failing.</li>
<li>Check for consistent <i>slurm.conf</i> and credential files on
the node(s) experiencing problems.</li>
<li>If this is user-specific problem, check that the user is 
configured on the controller computer(s) as well as the 
compute nodes.
The user doesn't need to be able to login, but his user ID 
must exist.</li>
<li>Check that a consistent version of SLURM exists on all of 
the nodes (execute "<i>sinfo -V</i>" or "<i>rpm -qa | grep slurm</i>").
If the first two digits of the version number match it should 
work fine, but version 1.1 commands will not work with 
version 1.2 daemons or vise-versa.</li> 
</ol>

<p style="text-align:center;">Last modified 12 October 2006</p>

<!--#include virtual="footer.txt"-->
