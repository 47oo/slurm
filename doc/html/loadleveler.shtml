<!--#include virtual="header.txt"-->

<h1><a name="top">SLURM Interface for LoadLeveler</a></h1>

<h2>Contents</h2>
<ul>
<li><a href=#overview>Overview</a></li>
<li><a href=#commands>Commands</a></li>
<ul>
<li><a href=#salloc>salloc</a></li>
<li><a href=#sbatch>sbatch</a></li>
<li><a href=#scancel>scancel</a></li>
<li><a href=#sinfo>sinfo</a></li>
<li><a href=#squeue>squeue</a></li>
<li><a href=#srun>srun</a></li>
<li><a href=#sstat>sstat</a></li>
</ul>
<li><a href=#install>Installation and Configuration</a></li>
</ul>

<h2><a name="overview">Overview</a></h2>

<p>SLURM version 2.4 has the ability to support a limited subset of SLURM
commands over a
<a href="http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/index.jsp?topic=/com.ibm.cluster.loadl.doc/llbooks.html">
LoadLeveler</a> infrastructure.
Note this is a distinct branch of the SLURM code rather than the default
SLURM distribution. This "loadleveler" branch is available from
<a href="https://github.com/SchedMD/slurm/tree/loadleveler">https://github.com/SchedMD/slurm/tree/loadleveler</a>.
The user is presented with a set of commands that look and feel like a native
SLURM installation, but which interface to an underlying LoadLeveler job scheduler.
Most changes are in the SLURM library which communicates with LoadLeveler rather
than the SLURM daemons to get system information and submit jobs.
The information in this document assumes a working knowledge of SLURM and is
meant to describe the differences in SLURM operation with LoadLeveler.</p>

<p>The supported commands include:
<a href=#salloc>salloc</a>,
<a href=#sbatch>sbatch</a>,
<a href=#scancel>scancel</a>,
<a href=#sinfo>sinfo</a>,
<a href=#squeue>squeue</a>,
<a href=#srun>srun</a>, and
<a href=#sstat>sstat</a>.
Details about each SLURM command's operation with LoadLeveler can be found
below, but there are some general differences which apply to many of the commands.
<ol>
<li><b>Hostnames</b>: LoadLeveler is typically configured with fully qualified
hostnames, which makes for rather verbose output. SLURM commands reports
hostnames to the first period (e.g. "tux123.cluster.corporation.com" becomes
"tux123").</li>
<li><b>Job ID</b>: LoadLeveler jobs can be submitted to multiple controller
daemons and a hostname is included as part of its job ID. SLURM commands were
modified to include the hostname in the job ID, so it is treated as a string
rather than a number in these SLURM commands. Note that the hostnames are
also truncated in the job ID (e.g. LoadLeveler job ID
"tux123.cluster.corporation.com.45678" is reported as "tux123.45678" by the
SLURM commands).</li>
<li><b>Job Steps</b>: Submitting multiple job steps to LoadLeveler requires
the batch script submitted contain syntax defining and submitting the job
steps. This mode of operation is not compatible with SLURM, which permits
job steps to be created an arbitrary times during the job's execution. SLURM's
srun command wrapper addresses this issue by invoking LoadLeveler's POE
command, but each invocation is treated like a single job step with ID 0.</li>
<li><b>Partitions</b>: LoadLeveler classes are treated as SLURM
partitions.</li>
<li><b>Node Resources</b>: LoadLeveler node resources are treated as
SLURM generic resources (GRES).</li>
</ol> 
</p>

<h2><a name="commands">Commands</a></h2>
<h3><a name="salloc">salloc</a></h3>
<p>LoadLeveler lacks an interactive job allocation mode equivalent to SLURM.
The salloc command submits a batch job to LoadLeveler that will make an
authenticated socket connection back to the salloc command once the resource 
allocation has been made as shown in Figure 1. This socket connection will be
used to spawn subsequent programs within the LoadLeveler job allocation;
specifically to invoke LoadLeveler's POE command in response to the invocation
of SLURM's srun command.</p>

<div class="figure">
  <a name="fig1"><img src=loadleveler_fig1.gif width=500></a><br>
  Figure 1. Salloc Command Operation
</div>

<p>Most salloc options function as expected to submit a job to LoadLeveler.
The following salloc options have no LoadLeveler equivalent and will be ignored:<p>
<ul>
<li>--acctg-freq (accounting polling interval, seconds)</li>
<li>--contiguous (set if job requires contiguous nodes)</li>
<li>--cores-per-socket (cores per socket required by job)</li>
<li>-d, --dependency (synchronize this job with other jobs)</li>
<li>-x, --exclude (nodes excluded from allocation request)</li>
<li>--gid (group to use, if run as root)</li>
<li>-I, --immediate (allocate to run or fail immediately)</li>
<li>--jobid (user-specified job ID)</li>
<li>-L, --licenses (licenses requiree by the job)</li>
<li>--mincpus (minimum number of processors required)</li>
<li>-k, --no-kill (set to not kill job upon node failure)</li>
<li>--ntasks-per-core (number of tasks to invoke per core)</li>
<li>--ntasks-per-socket (number of tasks to invoke per socket)</li>
<li>-O, --overcommit (over subscribe resources)</li>
<li>--qos (Quality of Service)</li>
<li>--signal (signal to send when near time limit and the time required)</li>
<li>--sockets-per-node (sockets per node required by job)</li>
<li>--switches (desired switch count and maximum delay)</li>
<li>--time-min (minimum run time in minutes)</li>
<li>--uid (user ID to use, if run as root)</li>
<li>--wait-all-nodes (wait for all nodes to boot before starting script, LoadLeveler default)</li>
<li>--wckey (workload characterization key)</li>
</ul>

<p>In addition, salloc some options are not fully compatable:
Only the only --cpu-bind supported are "cores" and "threads", all --mem-bind
options are mapped to LoadLeveler's "mcm_affinity_options=mcm_mem_req".</p>

<p>The following environment variables are set for salloc jobs:
SLURM_JOBID, SLURM_JOB_CPUS_PER_NODE, SLURM_JOB_ID, SLURM_JOB_NODELIST,
SLURM_JOB_NUM_NODES, SLURM_NNODES, SLURM_NODELIST, SLURM_SUBMIT_DIR, and
SLURM_TASKS_PER_NODE.
In addition, the following two environment variables are used for communications
between the salloc command and a batch job submitted to LoadLeveler:
SLURM_BE_KEY, SLURM_BE_SOCKET.</p>

<p>If SLURM's srun command is executed within an
salloc command, an authenticated communication will be sent to the salloc
back-end process to spawn an srun back-end process, which then spawns the
LoadLeveler POE command, which will launch the user tasks as shown in Figure 2.
Note that the srun back-end process manages standard input, output and error
file traffic back to the srun which initiated it.</p>

<div class="figure">
  <a name="fig2"><img src=loadleveler_fig2.gif width=500></a><br>
  Figure 2. Srun Command Operation under Salloc
</div>

<h3><a name="sbatch">sbatch</a></h3>
<p>Most sbatch options function as expected to submit a job to LoadLeveler.
The following sbatch options have no LoadLeveler equivalent and will be ignored:<p>

<ul>
<li>--acctg-freq (accounting polling interval, seconds)</li>
<li>--contiguous (set if job requires contiguous nodes)</li>
<li>--cores-per-socket (cores per socket required by job)</li>
<li>-d, --dependency (synchronize this job with other jobs)</li>
<li>-x, --exclude (nodes excluded from allocation request)</li>
<li>--gid (group to use, if run as root)</li>
<li>-I, --immediate (allocate to run or fail immediately)</li>
<li>--jobid (user-specified job ID)</li>
<li>-L, --licenses (licenses requiree by the job)</li>
<li>--mincpus (minimum number of processors required)</li>
<li>-k, --no-kill (set to not kill job upon node failure)</li>
<li>--ntasks-per-core (number of tasks to invoke per core)</li>
<li>--ntasks-per-socket (number of tasks to invoke per socket)</li>
<li>--open_mode (stdout/err open mode truncate or append)</li>
<li>-O, --overcommit (over subscribe resources)</li>
<li>--qos (Quality of Service)</li>
<li>--signal (signal to send when near time limit and the time required)</li>
<li>--sockets-per-node (sockets per node required by job)</li>
<li>--switches (desired switch count and maximum delay)</li>
<li>--time-min (minimum run time in minutes)</li>
<li>--uid (user ID to use, if run as root)</li>
<li>--wait-all-nodes (wait for all nodes to boot before starting script, LoadLeveler default)</li>
<li>--wckey (workload characterization key)</li>
</ul>

<p>In addition, sbatch some options are not fully compatable:
Only the only --cpu-bind supported are "cores" and "threads", all --mem-bind
options are mapped to LoadLeveler's "mcm_affinity_options=mcm_mem_req".</p>

<p>Note that some LoadLeveler options lack a SLURM equivalent. Those options may
be specified in the batch job submitted to sbatch. The first line of that
script should identify the shell to interpret the script. Subsequent lines
starting with "# @" and including LoadLeveler options will be left for
LoadLeveler to interpret. For example:</p>

<pre>
#!/bin/bash
# @ bulkxfer = yes
srun a.out
</pre>


<h3><a name="scancel">scancel</a></h3>
<p>LoadLeveler lacks the ability to send an arbitrary signal to any job or
job step. It is only able to cancel specific a specific job. Any attempt to
use scancel for signalling a job step or sending a signal other than SIGKILL
to a job will result in an error message of "Requested operation not supported
on this system". scancel's job filtering options however do work as expected
(--account, --interactive, --name, --partition, --reservation, --state and
--user).</p>


<h3><a name="sinfo">sinfo</a></h3>
<p>All sinfo options function as expected including filtering, sorting, and
output formatting.
LoadLeveler job classes are reported as a SLURM partition.
LoadLevler node resources are reported as SLURM generic resources.
Several sinfo output fields are unavailable due to lack of equivalent node
or partition information in LoadLeveler (details below).
The following sinfo fields are unavailable on LoadLeveler systems (listed
by output format field specification):
<ul>
<li>%E - Reason for a node being unavailable (sorted by time)</li>
<li>%g - Groups that can use the partition/class</li>
<li>%R - Time when a node became unavailable</li>
<li>%R - Reason for a node being unavailable (sorted by reason string)</li>
<li>%s - Maximum job size in nodes</li>
<li>%S - Nodes from which jobs may be submitted</li>
<li>%u - Name of user who made a node unavailable</li>
<li>%U - Name and ID of user who made a node unavailable</li>
<li>%w - Scheduling weight/priority nodes</li>
<li>%X - Number of sockets per node</li>
<li>%Y - Number of cores per socket</li>
<li>%Z - Number of threads per core</li>
<li>%z - Number of sockets, cores and threads per node</li>
</ul>
<!-- <p>The following SLURM partition information lacks an equivalant in LoadLeveler:
allow_alloc_nodes (nodes which can submit jobs to a partition),
alternate (partition to which jobs are transferred if this partition is unable
to accept new job requests),
def_mem_per_cpu, def_mem_per_node, max_mem_per_cpu, max_mem_per_node
(default and maximum memory limits for jobs),
max_nodes (maximum job size in nodes).
The following SLURM node information lack an equivalant in LoadLeveler:
cores, sockets, threads (only CPU counts available),
boot_time, slurmd_start_time,
reason, reason_time, reason_uid, and weight.</p> -->


<h3><a name="squeue">squeue</a></h3>
<p>All squeue options function as expected including filtering, sorting, and
output formatting.
The default field sizes for job ID and step ID have been increased by ten
characters to provided the space required for a hostname component.
LoadLeveler job classes are reported as a SLURM partition.
LoadLevler node resources are reported as SLURM generic resources.
The following sinfo fields are unavailable on LoadLeveler systems (listed
by output format field specification):
<ul>
<li>%c - CPUs per node required</li>
<li>%d - Temporary disk space required</li>
<li>%f - Node features required</li>
<li>%H - Sockets per node required</li>
<li>%I - Cores per socket required</li>
<li>%J - Threads per core required</li>
<li>%m - Memory required</li>
<li>%n - Names of nodes to be included in the resource allocation</li>
<li>%O - Contiguous nodes required</li>
<li>%q - Quality Of Service (QOS)</li>
<li>%s - Node select plugin specific information</li>
<li>%w - Workload Charactization Key</li>
<li>%W - Licenses required</li>
<li>%x - Names of nodes to be excluded from the resource allocation</li>
<li>%z - Number of sockets, cores and threads required per node</li>
</ul>
<!-- <p>In addition the following SLURM job information lacks an equivalant in
LoadLeveler:
alloc_sid (allocation session ID), assoc_id (association ID),
batch_script, nice, cpus_per_task, pn_min_cpus (per node minimum CPUs),
eligible_time, preempt_time, pre_sus_time (time used prior to last job suspend),
resize_time, suspend_time, req_switch (requested switch count),
wait4switch (how long to wait for requested switch count), and
job_resrcs (job allocated resources details, e.g. specific cores).</p> -->


<h3><a name="srun">srun</a></h3>
<p>LoadLeveler lacks an interactive job allocation mode equivalent to SLURM.
The srun command submits a batch job to LoadLeveler that will make a socket
connection back to the srun command once the resource allocation has been
made using the same mechanism as the salloc command shown in
<a href="#fig2">Figure 2</a> except the salloc back-end is invoked directly
by srun rather than the salloc command.</p>

<p>Most srun options function as expected to submit a job to LoadLeveler.
The following srun options have no LoadLeveler equivalent and will be ignored:<p>
<ul>
<li>--acctg-freq (accounting polling interval, seconds)</li>
<li>--contiguous (set if job requires contiguous nodes)</li>
<li>--cores-per-socket (cores per socket required by job)</li>
<li>-d, --dependency (synchronize this job with other jobs)</li>
<li>--epilog</li>
<li>-x, --exclude (nodes excluded from allocation request)</li>
<li>--gid (group to use, if run as root)</li>
<li>-I, --immediate (allocate to run or fail immediately)</li>
<li>--jobid (user-specified job ID)</li>
<li>-K, --kill-on-bad-exit (terminate step if any task has non-zero exit code</li>
<li>-L, --licenses (licenses requiree by the job)</li>
<li>--mincpus (minimum number of processors required)</li>
<li>--msg-timeout (message timeout)</li>
<li>--mpi (MPI type)</li>
<li>-k, --no-kill (set to not kill job upon node failure)</li>
<li>--ntasks-per-core (number of tasks to invoke per core)</li>
<li>--ntasks-per-socket (number of tasks to invoke per socket)</li>
<li>--open_mode (stdout/err open mode truncate or append)</li>
<li>-O, --overcommit (over subscribe resources)</li>
<li>--prolog</li>
<li>-q, --quit-on-interrupt</li>
<li>--qos (Quality of Service)</li>
<li>-r, --relative (resource allocation offset within the job's allocation)</li>
<li>--resv-ports (Communication ports reserved for OpenMPI)</li>
<li>--runjob-opts (Used only on IBM BlueGene/Q systems)</li>
<li>--signal (signal to send when near time limit and the time required)</li>
<li>--slurmd-debug (SLURM daemon debugging option)</li>
<li>--sockets-per-node (sockets per node required by job)</li>
<li>--switches (desired switch count and maximum delay)</li>
<li>--task-epilog (per-task epilog script)</li>
<li>--task-prolog (per-task prolog script)</li>
<li>--test-only (report estimated start time)</li>
<li>--time-min (minimum run time in minutes)</li>
<li>-u, --unbuffered (avoid line buffering)</li>
<li>--uid (user ID to use, if run as root)</li>
<li>-W, --wait (Specifies job wait time after first task exit)</li>
<li>--wckey (workload characterization key)</li>
<li>-X, --disable-status (disable task status reports with SIGINT)</li>
<li>-Z, --no-allocate (launch tasks without creating job allocation)</li>
</ul>

<p>In addition, srun some options are not fully compatable:
Only the only --cpu-bind supported are "cores" and "threads", all --mem-bind
options are mapped to LoadLeveler's "mcm_affinity_options=mcm_mem_req".</p>

<p>When the srun command is invoked from within an existing allocation,
it causes LoadLeveler's POE command to be invoked and a job step to be created.
Since it is not possible for the job steps to be defined at the time that a
batch job is submitted to LoadLeveler, every srun invocation will be treated
as one job step with an ID of zero.</p>

<h3><a name="sstat">sstat</a></h3>
<p>All sstat options function as expected including filtering, sorting, and
output formatting, however only a limited subset of information is available.
In fact, he only information available is the job step ID, CPU time used by
the job step, and task count.
The following sstat data is not available from LoadLeveler:
pid (process ID),
AveVMSize, MaxVMSize, MaxVMSizeNode, MaxVMSizeTask (Virtual Memory data),
AveRSS, MaxRSS, MaxRSSNode, MaxRSSTask (Resident Set Size, real memory data),
AvePages, MaxPages, MaxPagesNode, and MaxPagesTask (paging data),
MinCPU, MinCPUNode, MinCPUTask (CPU time information by node or task).
While the LoadLeveler API defines functions to read Resident Set Size (real
memory) information, only values of zero have been observed for running job
steps.</p>

<h2><a name=install>Installation and Configuration</a></h2>

<p>Because of the extensive changes required to the SLURM code for LoadLeveler
support, this code is maintained in the "loadleveler" branch of the SLURM code
rather than in the "master" SLURM code base. This "loadleveler" branch is
directly available from
<a href="https://github.com/SchedMD/slurm/tree/loadleveler">https://github.com/SchedMD/slurm/tree/loadleveler</a>.
The source code tar-ball and RPMs can be made available to SchedMD customers
upon request.</p>

<p>Building SLURM files to support LoadLeveler requires that SLURM's "configure"
program (executed at build time) be invoked with the option
"--with-loadleveler=&lt;path&gt;" where "&lt;path&gt;" identifies the
fully qualified pathname of where LoadLeveler is installed (i.e. the "llapi.h"
header file will be at "&lt;path&gt;/include/llapi.h"). If SLURM's "configure"
program is invoked by the "rpmbuild" command, then in your "~/.rpmmacros" file
include the line<br>
"%_with_loadleveler &lt;path&gt;" for the same result.
Only one RPM file will be created with all of the required files.</p>

<p>There is no need for any SLURM configuration files when operating with
LoadLeveler.</p>

<p class="footer"><a href="#top">top</a></p>

<p style="text-align:center;">Last modified 21 Febuary 2012</p>

<!--#include virtual="footer.txt"-->
